# 代码编写规范<a name="modelarts_23_0154"></a>

## 代码约束<a name="section740312011481"></a>

-   目前，自动化搜索作业的AI引擎仅支持“AutoNet-python2.7“。“AutoNet-python2.7“为ModelArts提供的引擎，内置了TensorFlow 1.13.0版本的引擎。在编写自动化搜索作业的代码时，需使用TensorFlow 1.13.0版本相应的接口。
-   ModelArts在运行用户代码时传入额外参数，所以请使用非排他性的入参解析API，例如使用“argparse“的“parse\_known\_args“替代“parse\_args“。
-   ModelArts会传入“nas\_code“作为运行一次用户代码的唯一标示，请务必将该唯一标示用于文件保存，以免出现多个任务并行下读写文件冲突，特别是在进行模型导出时。
-   在启动代码中，需要提供命名为“train“的方法，由ModelArts调用进行一次完整的训练，评估与模型导出。如果训练与评估是分开的，可以在该代码中提供一个命名为“eval“的方法，作为模型评估与导出的入口，ModelArts会在“train“执行完毕后在同一个进程里接着调用“eval“，“train“和“eval“方法都没有入参。

## NAS搜索<a name="section3766063610"></a>

1.  根据自己选择的搜索空间（例如restnet50的backbone部分、目标检测中的金字塔部分等等），在代码中调用autonet包内的相应API替换掉自己的构图代码。例如autonet.resnet.resnet50，该API会返回一个经过搜索后的resnet50网络运算后的tensor，后续对该tensor所进行的运算，和不使用autonet服务的场景一样不用做改动。
2.  最后在整个训练完成后，调用autonet.report，将您关心的指标（比如精度，推理时延等），报告给上层服务做汇总与决策。

示例代码请参见[使用示例](使用示例.md)章节的步骤[•纯NAS启动配置](使用示例.md#li0879105495919)。

## 超参-多元搜索（可选）<a name="section16924164915514"></a>

如果用户需要获得更好的搜索效果，即针对每组NAS搜索结果都使用契合该网络结构的超参进行训练，则可以使用多元搜索能力，需要额外进行以下操作。

1.  根据实际需求，准备好一个yaml配置文件，详细编写说明请参见[yaml配置文件说明](yaml配置文件说明.md)。
2.  搜索得到的参数值会通过“sys.argv“的途径传递给代码，可以通过“argparse“或者“flag“的形式，接收用户在yaml里定义好的超参，并在用户定义的训练代码中使用此超参。
3.  在训练的每一次迭代完成后，调用“hp\_search.metric“接口向上层服务报告超参搜索的目标指标，例如loss，用于进行任务调度与迭代，目标指标及对应的入参名字也是在yaml配置中指定的。

示例代码请参见[使用示例](使用示例.md)章节的步骤[•多元搜索\(超参+NAS\)启动配置](使用示例.md#li9961961100)。

